{
  List<FreqProxTermsWriterPerField> allFields=new ArrayList<FreqProxTermsWriterPerField>();
  for (  Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {
    Collection<TermsHashConsumerPerField> fields=entry.getValue();
    for (    final TermsHashConsumerPerField i : fields) {
      final FreqProxTermsWriterPerField perField=(FreqProxTermsWriterPerField)i;
      if (perField.termsHashPerField.numPostings > 0)       allFields.add(perField);
    }
  }
  CollectionUtil.quickSort(allFields);
  final int numAllFields=allFields.size();
  final FormatPostingsFieldsConsumer consumer=new FormatPostingsFieldsWriter(state,fieldInfos);
  try {
    int start=0;
    while (start < numAllFields) {
      final FieldInfo fieldInfo=allFields.get(start).fieldInfo;
      final String fieldName=fieldInfo.name;
      int end=start + 1;
      while (end < numAllFields && allFields.get(end).fieldInfo.name.equals(fieldName))       end++;
      FreqProxTermsWriterPerField[] fields=new FreqProxTermsWriterPerField[end - start];
      for (int i=start; i < end; i++) {
        fields[i - start]=allFields.get(i);
        if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
          fieldInfo.storePayloads|=fields[i - start].hasPayloads;
        }
      }
      appendPostings(fieldName,state,fields,consumer);
      for (int i=0; i < fields.length; i++) {
        TermsHashPerField perField=fields[i].termsHashPerField;
        int numPostings=perField.numPostings;
        perField.reset();
        perField.shrinkHash(numPostings);
        fields[i].reset();
      }
      start=end;
    }
    for (    Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {
      FreqProxTermsWriterPerThread perThread=(FreqProxTermsWriterPerThread)entry.getKey();
      perThread.termsHashPerThread.reset(true);
    }
  }
  finally {
    consumer.finish();
  }
}
