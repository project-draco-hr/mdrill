{
  if (!removeStopFilter) {
    return queryAnalyzer.tokenStream(fieldName,reader);
  }
  Analyzer a=map.get(fieldName);
  if (a != null) {
    return a.tokenStream(fieldName,reader);
  }
  FieldType ft=parser.getReq().getSchema().getFieldType(fieldName);
  Analyzer qa=ft.getQueryAnalyzer();
  if (!(qa instanceof TokenizerChain)) {
    map.put(fieldName,qa);
    return qa.tokenStream(fieldName,reader);
  }
  TokenizerChain tcq=(TokenizerChain)qa;
  Analyzer ia=ft.getAnalyzer();
  if (ia == qa || !(ia instanceof TokenizerChain)) {
    map.put(fieldName,qa);
    return qa.tokenStream(fieldName,reader);
  }
  TokenizerChain tci=(TokenizerChain)ia;
  for (  TokenFilterFactory tf : tci.getTokenFilterFactories()) {
    if (tf instanceof StopFilterFactory) {
      map.put(fieldName,qa);
      return qa.tokenStream(fieldName,reader);
    }
  }
  int stopIdx=-1;
  TokenFilterFactory[] facs=tcq.getTokenFilterFactories();
  for (int i=0; i < facs.length; i++) {
    TokenFilterFactory tf=facs[i];
    if (tf instanceof StopFilterFactory) {
      stopIdx=i;
      break;
    }
  }
  if (stopIdx == -1) {
    map.put(fieldName,qa);
    return qa.tokenStream(fieldName,reader);
  }
  TokenFilterFactory[] newtf=new TokenFilterFactory[facs.length - 1];
  for (int i=0, j=0; i < facs.length; i++) {
    if (i == stopIdx)     continue;
    newtf[j++]=facs[i];
  }
  TokenizerChain newa=new TokenizerChain(tcq.getTokenizerFactory(),newtf);
  newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));
  map.put(fieldName,newa);
  return newa.tokenStream(fieldName,reader);
}
