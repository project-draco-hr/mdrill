{
  Map stormconf=Utils.readStormConfig();
  Integer shards=StormUtils.parseInt(stormconf.get("higo.shards.count"));
  String hdfsSolrDir=(String)stormconf.get("higo.table.path");
  Configuration conf=getConf(stormconf);
  String tablename=args[1];
  String inputdir=args[2];
  Integer maxday=Integer.parseInt(args[3]);
  String startday=args[4];
  String format="seq";
  if (args.length > 5) {
    format=args[5];
  }
  String split="";
  if (args.length > 6) {
    split=args[6];
  }
  String submatch="*";
  if (args.length > 7) {
    submatch=args[7];
  }
  String rep=conf.get("dfs.replication");
  if (args.length > 8) {
    rep=args[8];
  }
  int dayplus=3;
  if (args.length > 9) {
    dayplus=Integer.parseInt(args[9]);
    ;
  }
  Integer parallel=StormUtils.parseInt(stormconf.get("higo.index.parallel"));
  if (stormconf.containsKey("higo.index.parallel." + tablename)) {
    parallel=StormUtils.parseInt(String.valueOf(stormconf.get("higo.index.parallel." + tablename)));
  }
  String tablemode="";
  if (stormconf.containsKey("higo.mode." + tablename)) {
    tablemode=String.valueOf(stormconf.get("higo.mode." + tablename));
  }
  String[] jobValues=new String[]{split,submatch,String.valueOf(parallel),tablemode,rep};
  String type=(String)stormconf.get("higo.partion.type");
  String tabletype=(String)stormconf.get("higo.partion.type." + tablename);
  if (tabletype != null && !tabletype.isEmpty()) {
    type=tabletype;
  }
  JobIndexerPartion index=new JobIndexerPartion(tablename,conf,shards,new Path(hdfsSolrDir,tablename).toString(),inputdir,dayplus,maxday,startday,format,type);
  ToolRunner.run(conf,index,jobValues);
  System.exit(0);
}
