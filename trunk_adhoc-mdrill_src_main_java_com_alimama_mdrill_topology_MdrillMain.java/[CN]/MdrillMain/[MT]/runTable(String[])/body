{
  String tableName=args[1];
  Map stormconf=Utils.readStormConfig();
  ClusterState zkClusterstate=Cluster.mk_distributed_cluster_state(stormconf);
  StormClusterState zkCluster=Cluster.mk_storm_cluster_state(zkClusterstate);
  zkCluster.higo_remove(tableName);
  zkCluster.disconnect();
  Integer shards=StormUtils.parseInt(stormconf.get("higo.shards.count"));
  Integer fixassign=StormUtils.parseInt(stormconf.containsKey("higo.fixed.shards") ? stormconf.get("higo.fixed.shards") : 0);
  Integer replication=StormUtils.parseInt(stormconf.containsKey("higo.shards.replication") ? stormconf.get("higo.shards.replication") : 1);
  Integer partions=StormUtils.parseInt(stormconf.get("higo.cache.partions"));
  Integer portbase=StormUtils.parseInt(stormconf.get("higo.solr.ports.begin"));
  String hdfsSolrDir=(String)stormconf.get("higo.table.path");
  String realtimebinlog=String.valueOf(stormconf.get("higo.realtime.binlog"));
  String topologyName="adhoc";
  String hadoopConfDir=(String)stormconf.get("hadoop.conf.dir");
  String localWorkDirList=(String)stormconf.get("higo.workdir.list");
  Integer msCount=StormUtils.parseInt(stormconf.get("higo.mergeServer.count"));
  Config conf=new Config();
  String[] tablelist=tableName.split(",");
  for (  String tbl : tablelist) {
    String key="higo.mode." + tbl;
    Object val=stormconf.get(key);
    if (val != null) {
      conf.put(key,val);
    }
  }
  conf.setNumWorkers(shards + msCount);
  conf.setNumAckers(1);
  conf.setMaxSpoutPending(100);
  conf.put(CustomAssignment.TOPOLOGY_CUSTOM_ASSIGNMENT,MdrillTaskAssignment.class.getName());
  conf.put(MdrillTaskAssignment.SHARD_REPLICATION,replication);
  conf.put(MdrillTaskAssignment.HIGO_FIX_SHARDS,fixassign);
  for (int i=1; i <= fixassign; i++) {
    conf.put(MdrillTaskAssignment.HIGO_FIX_SHARDS + "." + i,(String)stormconf.get("higo.fixed.shards" + "." + i));
  }
  conf.put(MdrillTaskAssignment.MS_PORTS,(String)stormconf.get("higo.merge.ports"));
  conf.put(MdrillTaskAssignment.REALTIME_PORTS,(String)stormconf.get("higo.realtime.ports"));
  conf.put(MdrillTaskAssignment.MS_NAME,"merge");
  conf.put(MdrillTaskAssignment.SHARD_NAME,"shard");
  conf.put(MdrillTaskAssignment.REALTIME_NAME,"realtime");
  BoltParams paramsMs=new BoltParams();
  paramsMs.compname="merge_0";
  paramsMs.replication=1;
  paramsMs.replicationindex=0;
  paramsMs.binlog=realtimebinlog;
  ShardsBolt ms=new ShardsBolt(paramsMs,hadoopConfDir,hdfsSolrDir,tableName,localWorkDirList,portbase + shards * replication + 100,0,partions,topologyName);
  TopologyBuilder builder=new TopologyBuilder();
  builder.setSpout("heartbeat",new HeartBeatSpout(),1);
  for (int i=0; i < replication; i++) {
    BoltParams paramsShard=new BoltParams();
    paramsShard.replication=replication;
    paramsShard.replicationindex=i;
    paramsShard.compname="shard_" + i;
    paramsShard.binlog=realtimebinlog;
    ShardsBolt solr=new ShardsBolt(paramsShard,hadoopConfDir,hdfsSolrDir,tableName,localWorkDirList,portbase + shards * i,shards,partions,topologyName);
    builder.setBolt("shard@" + i,solr,shards).allGrouping("heartbeat");
  }
  builder.setBolt("merge",ms,msCount).allGrouping("heartbeat");
  StormSubmitter.submitTopology(topologyName,conf,builder.createTopology());
  System.out.println("start complete ");
  System.exit(0);
}
