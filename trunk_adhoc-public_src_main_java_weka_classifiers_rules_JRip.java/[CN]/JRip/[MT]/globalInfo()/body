{
  return "This class implements a propositional rule learner, Repeated Incremental " + "Pruning to Produce Error Reduction (RIPPER), which was proposed by William " + "W. Cohen as an optimized version of IREP. \n\n"+ "The algorithm is briefly described as follows: \n\n"+ "Initialize RS = {}, and for each class from the less prevalent one to "+ "the more frequent one, DO: \n\n"+ "1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) "+ "of the ruleset and examples is 64 bits greater than the smallest DL "+ "met so far, or there are no positive examples, or the error rate >= 50%. "+ "\n\n"+ "1.1. Grow phase:\n"+ "Grow one rule by greedily adding antecedents (or conditions) to "+ "the rule until the rule is perfect (i.e. 100% accurate).  The "+ "procedure tries every possible value of each attribute and selects "+ "the condition with highest information gain: p(log(p/t)-log(P/T))."+ "\n\n"+ "1.2. Prune phase:\n"+ "Incrementally prune each rule and allow the pruning of any "+ "final sequences of the antecedents;"+ "The pruning metric is (p-n)/(p+n) -- but it's actually "+ "2p/(p+n) -1, so in this implementation we simply use p/(p+n) "+ "(actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n"+ "2. Optimization stage:\n after generating the initial ruleset {Ri}, "+ "generate and prune two variants of each rule Ri from randomized data "+ "using procedure 1.1 and 1.2. But one variant is generated from an "+ "empty rule while the other is generated by greedily adding antecedents "+ "to the original rule. Moreover, the pruning metric used here is "+ "(TP+TN)/(P+N)."+ "Then the smallest possible DL for each variant and the original rule "+ "is computed.  The variant with the minimal DL is selected as the final "+ "representative of Ri in the ruleset."+ "After all the rules in {Ri} have been examined and if there are still "+ "residual positives, more rules are generated based on the residual "+ "positives using Building Stage again. \n"+ "3. Delete the rules from the ruleset that would increase the DL of the "+ "whole ruleset if it were in it. and add resultant ruleset to RS. \n"+ "ENDDO\n\n"+ "Note that there seem to be 2 bugs in the original ripper program that would "+ "affect the ruleset size and accuracy slightly.  This implementation avoids "+ "these bugs and thus is a little bit different from Cohen's original "+ "implementation. Even after fixing the bugs, since the order of classes with "+ "the same frequency is not defined in ripper, there still seems to be "+ "some trivial difference between this implementation and the original ripper, "+ "especially for audiology data in UCI repository, where there are lots of "+ "classes of few instances.\n\n"+ "Details please see:\n\n" + getTechnicalInformation().toString() + "\n\n"+ "PS.  We have compared this implementation with the original ripper "+ "implementation in aspects of accuracy, ruleset size and running time "+ "on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these "+ "aspects it seems to be quite comparable to the original ripper "+ "implementation.  However, we didn't consider memory consumption "+ "optimization in this implementation.\n\n";
}
